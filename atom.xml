<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://casperit.github.io</id>
    <title>我的互联网杂记</title>
    <updated>2020-12-28T10:44:00.145Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://casperit.github.io"/>
    <link rel="self" href="https://casperit.github.io/atom.xml"/>
    <logo>https://casperit.github.io/images/avatar.png</logo>
    <icon>https://casperit.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, 我的互联网杂记</rights>
    <entry>
        <title type="html"><![CDATA[HRegionServer 启动过程]]></title>
        <id>https://casperit.github.io/post/hregionserver-qi-dong-guo-cheng/</id>
        <link href="https://casperit.github.io/post/hregionserver-qi-dong-guo-cheng/">
        </link>
        <updated>2020-12-22T12:43:23.000Z</updated>
        <content type="html"><![CDATA[<h2 id="1-调用链">1.  调用链</h2>
<pre><code>HRegionServerCommandLine.start()
  |
  |
       local
       -----
      |  silently exit
                
        cluster
        -----  
      | - HRegionServer.constructRegionServer()
      |   |- UserProvider.instantiate(conf)  // 初始化 userprovider
      |   |- FSUtils.setupShortCircuitRead(this.conf) // short circuit read setup
      |   |- createRpcServices() 
      |   |   | -  MasterRpcServices.init() -&gt;RSRpcServices.init()
      |   |   |   | - rpcServer = new RpcServer()
      |   |   |   | - rpcSchedulerFactory.create() 
      |   |   |   |   |   | - callExecutor //初始化读写executor
      |   |   |   |   |   | - priorityExecutor //初始化priorityExecutor
      |   |   |   |   |   | - replicationExecutor //初始化replicationExecutor
      |   |-  new ZooKeeperWatcher() //初始化zk连接和watcher，并create相关的znode
      |   |   | - createBaseZNodes()
      |   |-  TableLockManager.createTableLockManager // 基于zk的table-level lock
      |   |-  new MasterAddressTracker.start() // znode watcher to track master address ( /hbase/master )
      |   |-  new ClusterStatusTracker.start() // znode watcher to track cluster status ( /hbase/running )
      |   |-  new ConfigurationManager() // reload configuration on the fly
      |   |-  rpcServices.start() // start rpc threads
      |   |    |-  rpcServices.start() // start rpc threads
      |   |    |    |-  responder.start()
      |   |    |    |-  listener.start()
      |   |    |    |-  scheduler.start()
      |   |    |    |    | -  callExecutor.start
      |   |    |    |    | -  priorityExecutor.start
      |   |    |    |    | -  replicationExecutor.start
      |    |   |    |-  putUpWebUI() 
      |    |   |    |-  new LogRoller()
      |    |   |    |-  new ChoreService()
</code></pre>
<h2 id="2-流程详解">2.  流程详解</h2>
<ol>
<li>
<p>HRegionServer 启动入口类为 hbase-server module 的 org.apache.hadoop.hbase.regionserver.HRegionServer</p>
</li>
<li>
<p>核心代码实现如下:</p>
<ol>
<li>调用入口类</li>
</ol>
<pre><code class="language-java">org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine#start


Configuration conf = getConf();
CoordinatedStateManager cp =      CoordinatedStateManagerFactory.getCoordinatedStateManager(conf);
try {
  // If 'local', don't start a region server here. Defer to
  // LocalHBaseCluster. It manages 'local' clusters.
  if (LocalHBaseCluster.isLocal(conf)) {
    LOG.warn(&quot;Not starting a distinct region server because &quot;
        + HConstants.CLUSTER_DISTRIBUTED + &quot; is false&quot;);
  } else {
    logProcessInfo(getConf());
    HRegionServer hrs = HRegionServer.constructRegionServer(regionServerClass, conf, cp);
    hrs.start();
    hrs.join();
    if (hrs.isAborted()) {
      throw new RuntimeException(&quot;HRegionServer Aborted&quot;);
    }
  }
} catch (Throwable t) {
  LOG.error(&quot;Region server exiting&quot;, t);
  return 1;
}
return 0;
</code></pre>
<ol start="2">
<li>HRegionServer 实例init</li>
</ol>
<pre><code class="language-java">org.apache.hadoop.hbase.regionserver.HRegionServer#HRegionServer


```java
org.apache.hadoop.hbase.regionserver.HRegionServer#HRegionServer

super(&quot;RegionServer&quot;);  // thread name

this.userProvider = UserProvider.instantiate(conf); // 初始化 user 实例
FSUtils.setupShortCircuitRead(this.conf);

this.nonceManager = isNoncesEnabled ? new ServerNonceManager(this.conf) : null; // nonceManager，通过 nonce 机制防止同一个请求被重复提交

rpcServices = createRpcServices(); //初始化 RpcServices 实例，包含scheduler,listener,responder和reader等对象

initializeFileSystem(); // 分别初始化wal和data HfileSystem，

service = new ExecutorService(getServerName().toShortString()); //初始化一个通用的executor service，包含一个threadpool和runnable
spanReceiverHost = SpanReceiverHost.getInstance(getConfiguration());

// Some unit tests don't need a cluster, so no zookeeper at all
if (!conf.getBoolean(&quot;hbase.testing.nocluster&quot;, false)) {
  // Open connection to zookeeper and set primary watcher
  zooKeeper = new ZooKeeperWatcher(conf, getProcessName() + &quot;:&quot; +
    rpcServices.isa.getPort(), this, canCreateBaseZNode());

  this.csm = (BaseCoordinatedStateManager) csm;
  this.csm.initialize(this);
  this.csm.start();

  tableLockManager = TableLockManager.createTableLockManager(
    conf, zooKeeper, serverName); //基于zk的tracker

  masterAddressTracker = new MasterAddressTracker(getZooKeeper(), this);
  masterAddressTracker.start(); //基于zk的tracker

  clusterStatusTracker = new ClusterStatusTracker(zooKeeper, this);
  clusterStatusTracker.start(); //基于zk的tracker
}
this.configurationManager = new ConfigurationManager(); // configurationManager to update certain configuration on the fly

rpcServices.start();  // start responder,listener,scheduler service
putUpWebUI(); // start http page
this.walRoller = new LogRoller(this, this); // wal rollger manager
this.choreService = new ChoreService(getServerName().toString(), true);
this.flushThroughputController = FlushThroughputControllerFactory.create(this, conf);

// 注册 HUP signal，用于 reload configuration 并通知相关 observer
if (!SystemUtils.IS_OS_WINDOWS) {
  Signal.handle(new Signal(&quot;HUP&quot;), new SignalHandler() {
    @Override
    public void handle(Signal signal) {
      getConfiguration().reloadConfiguration();
      configurationManager.notifyAllObservers(getConfiguration());
    }
  });
}
// Create the CompactedFileDischarger chore service. This chore helps to
// remove the compacted files
// that will no longer be used in reads.
// Default is 2 mins. The default value for TTLCleaner is 5 mins so we set this to
// 2 mins so that compacted files can be archived before the TTLCleaner runs
int cleanerInterval = conf
    .getInt(CompactionConfiguration.HBASE_HFILE_COMPACTION_DISCHARGER_INTERVAL, 2 * 60 * 1000);
this.compactedFileDischarger =
    new CompactedHFilesDischarger(cleanerInterval, (Stoppable)this, (RegionServerServices)this);
choreService.scheduleChore(compactedFileDischarger);
</code></pre>
<ol start="3">
<li>HRegionServer start: 具体执行的就是对应类的run方法</li>
</ol>
<pre><code class="language-java">org.apache.hadoop.hbase.regionserver.HRegionServer#run

{
try {
  // Do pre-registration initializations; zookeeper, lease threads, etc.
  preRegistrationInitialization();  // 在向master注册前，需要进行初始化，详细参考第四点
} catch (Throwable e) {
  abort(&quot;Fatal exception during initialization&quot;, e);
}

try {
  if (!isStopped() &amp;&amp; !isAborted()) {
    ShutdownHook.install(conf, fs, this, Thread.currentThread());
    // Initialize the RegionServerCoprocessorHost now that our ephemeral
    // node was created, in case any coprocessors want to use ZooKeeper
    this.rsHost = new RegionServerCoprocessorHost(this, this.conf);
  }

  // Try and register with the Master; tell it we are here.  Break if
  // server is stopped or the clusterup flag is down or hdfs went wacky.
  while (keepLooping()) {                    
    RegionServerStartupResponse w = reportForDuty();  // 建立与master的连接，并从maste端获取配置（key-value的map）
    if (w == null) {
      LOG.warn(&quot;reportForDuty failed; sleeping and then retrying.&quot;);
      this.sleeper.sleep();
    } else {
      handleReportForDutyResponse(w);  // Run init. Sets up wal and starts up all server threads
      break;
    }
  }

  if (!isStopped() &amp;&amp; isHealthy()){
    // start the snapshot handler and other procedure handlers,
    // since the server is ready to run
    rspmHost.start();
  }

  // Start the Quota Manager
  if (this.rsQuotaManager != null) {
    rsQuotaManager.start(getRpcServer().getScheduler());
  }

  // We registered with the Master.  Go into run mode.
  long lastMsg = System.currentTimeMillis();
  long oldRequestCount = -1;
  // The main run loop.
  while (!isStopped() &amp;&amp; isHealthy()) {
    if (!isClusterUp()) {
      if (isOnlineRegionsEmpty()) {
        stop(&quot;Exiting; cluster shutdown set and not carrying any regions&quot;);
      } else if (!this.stopping) {
        this.stopping = true;
        LOG.info(&quot;Closing user regions&quot;);
        closeUserRegions(this.abortRequested);
      } else if (this.stopping) {
        boolean allUserRegionsOffline = areAllUserRegionsOffline();
        if (allUserRegionsOffline) {
          // Set stopped if no more write requests tp meta tables
          // since last time we went around the loop.  Any open
          // meta regions will be closed on our way out.
          if (oldRequestCount == getWriteRequestCount()) {
            stop(&quot;Stopped; only catalog regions remaining online&quot;);
            break;
          }
          oldRequestCount = getWriteRequestCount();
        } else {
          // Make sure all regions have been closed -- some regions may
          // have not got it because we were splitting at the time of
          // the call to closeUserRegions.
          closeUserRegions(this.abortRequested);
        }
        LOG.debug(&quot;Waiting on &quot; + getOnlineRegionsAsPrintableString());
      }
    }
    long now = System.currentTimeMillis();
    if ((now - lastMsg) &gt;= msgInterval) {
      tryRegionServerReport(lastMsg, now);
      lastMsg = System.currentTimeMillis();
    }
    if (!isStopped() &amp;&amp; !isAborted()) {
      this.sleeper.sleep();
    }
  } // for
} catch (Throwable t) {
  if (!rpcServices.checkOOME(t)) {
    String prefix = t instanceof YouAreDeadException? &quot;&quot;: &quot;Unhandled: &quot;;
    abort(prefix + t.getMessage(), t);
  }
}
// Run shutdown.
if (mxBean != null) {
  MBeanUtil.unregisterMBean(mxBean);
  mxBean = null;
}
if (this.leases != null) this.leases.closeAfterLeasesExpire();
if (this.splitLogWorker != null) {
  splitLogWorker.stop();
}
if (this.infoServer != null) {
  LOG.info(&quot;Stopping infoServer&quot;);
  try {
    this.infoServer.stop();
  } catch (Exception e) {
    LOG.error(&quot;Failed to stop infoServer&quot;, e);
  }
}
// Send cache a shutdown.
if (cacheConfig != null &amp;&amp; cacheConfig.isBlockCacheEnabled()) {
  cacheConfig.getBlockCache().shutdown();
}

if (movedRegionsCleaner != null) {
  movedRegionsCleaner.stop(&quot;Region Server stopping&quot;);
}

// Send interrupts to wake up threads if sleeping so they notice shutdown.
// TODO: Should we check they are alive? If OOME could have exited already
if (this.hMemManager != null) this.hMemManager.stop();
if (this.cacheFlusher != null) this.cacheFlusher.interruptIfNecessary();
if (this.compactSplitThread != null) this.compactSplitThread.interruptIfNecessary();
sendShutdownInterrupt();

// Stop the quota manager
if (rsQuotaManager != null) {
  rsQuotaManager.stop();
}

// Stop the snapshot and other procedure handlers, forcefully killing all running tasks
if (rspmHost != null) {
  rspmHost.stop(this.abortRequested || this.killed);
}

if (this.killed) {
  // Just skip out w/o closing regions.  Used when testing.
} else if (abortRequested) {
  if (this.fsOk) {
    closeUserRegions(abortRequested); // Don't leave any open file handles
  }
  LOG.info(&quot;aborting server &quot; + this.serverName);
} else {
  closeUserRegions(abortRequested);
  LOG.info(&quot;stopping server &quot; + this.serverName);
}

// so callers waiting for meta without timeout can stop
if (this.metaTableLocator != null) this.metaTableLocator.stop();
if (this.clusterConnection != null &amp;&amp; !clusterConnection.isClosed()) {
  try {
    this.clusterConnection.close();
  } catch (IOException e) {
    // Although the {@link Closeable} interface throws an {@link
    // IOException}, in reality, the implementation would never do that.
    LOG.warn(&quot;Attempt to close server's short circuit HConnection failed.&quot;, e);
  }
}

// Closing the compactSplit thread before closing meta regions
if (!this.killed &amp;&amp; containsMetaTableRegions()) {
  if (!abortRequested || this.fsOk) {
    if (this.compactSplitThread != null) {
      this.compactSplitThread.join();
      this.compactSplitThread = null;
    }
    closeMetaTableRegions(abortRequested);
  }
}

if (!this.killed &amp;&amp; this.fsOk) {
  waitOnAllRegionsToClose(abortRequested);
  LOG.info(&quot;stopping server &quot; + this.serverName +
    &quot;; all regions closed.&quot;);
}

//fsOk flag may be changed when closing regions throws exception.
if (this.fsOk) {
  shutdownWAL(!abortRequested);
}

// Make sure the proxy is down.
if (this.rssStub != null) {
  this.rssStub = null;
}
if (this.rpcClient != null) {
  this.rpcClient.close();
}
if (this.leases != null) {
  this.leases.close();
}
if (this.pauseMonitor != null) {
  this.pauseMonitor.stop();
}

if (!killed) {
  stopServiceThreads();
}

if (this.rpcServices != null) {
  this.rpcServices.stop();
}

try {
  deleteMyEphemeralNode();
} catch (KeeperException.NoNodeException nn) {
} catch (KeeperException e) {
  LOG.warn(&quot;Failed deleting my ephemeral node&quot;, e);
}
// We may have failed to delete the znode at the previous step, but
//  we delete the file anyway: a second attempt to delete the znode is likely to fail again.
ZNodeClearer.deleteMyEphemeralNodeOnDisk();

if (this.zooKeeper != null) {
  this.zooKeeper.close();
}
LOG.info(&quot;stopping server &quot; + this.serverName +
  &quot;; zookeeper connection closed.&quot;);

LOG.info(Thread.currentThread().getName() + &quot; exiting&quot;);
}
</code></pre>
<ol start="4">
<li>preRegistrationInitialization()</li>
</ol>
<pre><code class="language-java">org.apache.hadoop.hbase.regionserver.HRegionServer#initializeThreads

{
this.cacheFlusher = new MemStoreFlusher(conf, this);  // memstoreflush thread

this.compactSplitThread = new CompactSplitThread(this); // compaction thread

// Background thread to check for compactions; needed if region has not gotten updates
// in a while. It will take care of not checking too frequently on store-by-store basis.
this.compactionChecker = new CompactionChecker(this, this.compactionCheckFrequency, this);
this.periodicFlusher = new PeriodicMemstoreFlusher(this.flushCheckFrequency, this);
this.leases = new Leases(this.threadWakeFrequency); // Lease manager

// Create the thread to clean the moved regions list
movedRegionsCleaner = MovedRegionsCleaner.create(this);

if (this.nonceManager != null) {
  // Create the scheduled chore that cleans up nonces.
  nonceManagerChore = this.nonceManager.createCleanupScheduledChore(this);
}

// Setup the Quota Manager
rsQuotaManager = new RegionServerQuotaManager(this);

// Setup RPC client for master communication
rpcClient = RpcClientFactory.createClient(conf, clusterId, new InetSocketAddress(
    rpcServices.isa.getAddress(), 0), clusterConnection.getConnectionMetrics());

boolean onlyMetaRefresh = false;
int storefileRefreshPeriod = conf.getInt(storefilerefresher thread
    StorefileRefresherChore.REGIONSERVER_STOREFILE_REFRESH_PERIOD
  , StorefileRefresherChore.DEFAULT_REGIONSERVER_STOREFILE_REFRESH_PERIOD);
if (storefileRefreshPeriod == 0) {
  storefileRefreshPeriod = conf.getInt(
      StorefileRefresherChore.REGIONSERVER_META_STOREFILE_REFRESH_PERIOD,
      StorefileRefresherChore.DEFAULT_REGIONSERVER_STOREFILE_REFRESH_PERIOD);
  onlyMetaRefresh = true;
}
if (storefileRefreshPeriod &gt; 0) {
  this.storefileRefresher = new StorefileRefresherChore(storefileRefreshPeriod,
      onlyMetaRefresh, this, this); // storefileRefresh chore线程
}
registerConfigurationObservers(); // 配置 reload 逻辑
}
</code></pre>
</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hmaster 启动过程]]></title>
        <id>https://casperit.github.io/post/hmaster-qi-dong-guo-cheng/</id>
        <link href="https://casperit.github.io/post/hmaster-qi-dong-guo-cheng/">
        </link>
        <updated>2020-12-15T10:52:50.000Z</updated>
        <content type="html"><![CDATA[<h2 id="1-调用链">1.  调用链</h2>
<pre><code>HMasterCommandLine
  |
  |
   - run               local
      |                -----
       - startMaster -|     |
                      |     |- new MiniZooKeeperCluster.startup //embedded zookeeper cluster
                      |     |   |- createDir //创建 zk 数据目录
                      |     |   |- zks = new ZooKeeperServer
                      |     |   |- new NIOServerCnxn.Factory(clientPort).startup //启动服务
                      |     |   |        |
                      |     |   |        |- zks.startdata
                      |     |   |        |    |
                      |     |   |        |    |- new ZKDatabase
                      |     |   |        |    |- loadData // Restore sessions and data
                      |     |   |        |
                      |     |   |         - zks.startup
                      |     |   |             |
                      |     |   |             |- startSessionTracker
                      |     |   |             |- setupRequestProcessors
                      |     |   |                   |
                      |     |   |                   |- new FinalRequestProcessor
                      |     |   |                   |- new SyncRequestProcessor.start
                      |     |   |                   |- new PrepRequestProcessor.start
                      |     |   |             |- registerJMX //注册JMX mbean
                      |     |   |
                      |     |    - waitForServerUp // echo &quot;stat&quot; to assert zk liveless
                      |     |
                      |     |
                      |      - new LocalHBaseCluster().startup //启动包括 Hmaster + RS 的服务
                      |         |
                      |         |- masterClass = org.apache.hadoop.hbase.master.HMasterCommandLine$LocalHMaster
                      |         |- addMaster()
                      |         |    |- createMasterThread()
                      |         |    |   |- HMaster.init()
                      |         |    |   |    |- HregionServer.init()
                      |         |    |   |    |   |- UserProvider.instantiate(conf)  // 初始化 userprovider
                      |         |    |   |    |   |- FSUtils.setupShortCircuitRead(this.conf) // short circuit read setup
                      |         |    |   |    |   |- createRpcServices() 
                      |         |    |   |    |   |   | -  MasterRpcServices.init() -&gt;RSRpcServices.init()
                      |         |    |   |    |   |   |   | - rpcServer = new RpcServer()
                      |         |    |   |    |   |   |   |   | - rpcSchedulerFactory.create() 
                      |         |    |   |    |   |   |   |   |   | - callExecutor //初始化读写executor
                      |         |    |   |    |   |   |   |   |   | - priorityExecutor //初始化priorityExecutor
                      |         |    |   |    |   |   |   |   |   | - replicationExecutor //初始化replicationExecutor
                      |         |    |   |    |   |-  new ZooKeeperWatcher() //初始化zk连接和watcher，并create相关的znode
                      |         |    |   |    |   |   | - createBaseZNodes()
                      |         |    |   |    |   |-  TableLockManager.createTableLockManager // 基于zk的table-level lock
                      |         |    |   |    |   |-  new MasterAddressTracker.start() // znode watcher to track master address ( /hbase/master )
                      |         |    |   |    |   |-  new ClusterStatusTracker.start() // znode watcher to track cluster status ( /hbase/running )
                      |         |    |   |    |   |-  new ConfigurationManager() // reload configuration on the fly
                      |         |    |   |    |   |-  rpcServices.start() // start rpc threads
                      |         |    |   |    |   |    |-  rpcServices.start() // start rpc threads
                      |         |    |   |    |   |    |    |-  responder.start()
                      |         |    |   |    |   |    |    |-  listener.start()
                      |         |    |   |    |   |    |    |-  scheduler.start()
                      |         |    |   |    |   |    |    |    | -  callExecutor.start
                      |         |    |   |    |   |    |    |    | -  priorityExecutor.start
                      |         |    |   |    |   |    |    |    | -  replicationExecutor.start
                      |         |    |   |    |   |    |-  putUpWebUI() 
                      |         |    |   |    |   |    |-  new LogRoller()
                      |         |    |   |    |   |    |-  new ChoreService()
                      |         |    |   |    | - new ActiveMasterManager() // Handles everything on master-side related to master election. This is a zk listener
                      |         |    |   |    | - startActiveMasterManager()
                      |         |    |   |    |   | - MasterAddressTracker.setMasterAddress() // create /hbase/backup-masters/XX
                      |         |    |   |    |   | -  Threads.setDaemonThreadRunning() // Start a new thread to try to become the active master 
                      |         |    |   |    |   |    |-   blockUntilBecomingActiveMaster() // try to become master
                      |         |    |   |    |   |    |  |-   finishActiveMasterInitialization() 
                      |         |    |   |    |   |    |  |  | - new MasterFileSystem() // class for hmaster to interact with underlying file system
                      |         |    |   |    |   |    |  |  | - new ServerManager()  // manage info about region servers including online/dead servers and processing the startsup,shutdown and deaths of regionservers
                      |         |    |   |    |   |    |  |  | - setupClusterConnection
                      |         |    |   |    |   |    |  |  | - initializeZKBasedSystemTrackers // Initialize all ZK based system trackers
                      |         |    |   |    |   |    |  |  |  | - getLoadBalancer
                      |         |    |   |    |   |    |  |  |  | - getRegionNormalizer
                      |         |    |   |    |   |    |  |  |  | - new AssignmentManager
                      |         |    |   |    |   |    |  |  |  | - new SnapshotManager
                      |         |    |   |    |   |    |  |  |  | - new MasterProcedureManagerHost
                      |         |    |   |    |   |    |  |  | - new MasterCoprocessorHost // master coprocessor class
                      |         |    |   |    |   |    |  |  | - startServiceThreads // start several master executor service
                      |         |    |   |    |   |    |  |  |  | - MASTER_OPEN_REGION
                      |         |    |   |    |   |    |  |  |  | - MASTER_CLOSE_REGION
                      |         |    |   |    |   |    |  |  |  | - MASTER_SERVER_OPERATIONS
                      |         |    |   |    |   |    |  |  |  | - MASTER_META_SERVER_OPERATIONS
                      |         |    |   |    |   |    |  |  |  | - M_LOG_REPLAY_OPS
                      |         |    |   |    |   |    |  |  |  | - MASTER_SNAPSHOT_OPERATIONS
                      |         |    |   |    |   |    |  |  |  | - MASTER_TABLE_OPERATIONS
                      |         |    |   |    |   |    |  |  |  | - new LogCleaner chore
                      |         |    |   |    |   |    |  |  |  | - new HFileCleaner chore
                      |         |    |   |    |   |    |  |  |  | - new replicationZKLockCleanerChore chore
                      |         |    |   |    |   |    |  |  |  | - new replicationZKNodeCleanerChore chore
                      |         |    |   |    |   |    |  |  | - assignMeta // assign meta table
                      |         |    |   |    |   |    |  |  | - new ClusterStatusChore()
                      |         |    |   |    |   |    |  |  | - new BalancerChore()
                      |         |    |   |    |   |    |  |  | - new RegionNormalizerChore()
                      |         |    |   |    |   |    |  |  | - new CatalogJanitor()
                      |         |    |   |    |   |    |  |  | - new PeriodicDoMetrics()
                      |         |    |   |    |   |    |  |  | - new TableNamespaceManager().start
                      |         |    |   |    |   |    |  |  | - configurationManager.registerObserver(this.balancer) //enable configuration reload on the fly
                      |         |    |   |    |   |    |  |  | - configurationManager.registerObserver(this.cleanerPool)
                      |         |    |   |    |   |    |  |  | - configurationManager.registerObserver(this.hfileCleaner)
                      |         |    |   |    |   |    |  |  | - configurationManager.registerObserver(this.logCleaner)
                      |         |    |   |    |   |    |  |  | - initQuotaManager
                      |         | - RegionServerClass = org.apache.hadoop.hbase.regionserver.HRegionServer
                      |         | - addRegionServer ()
                      |         |    |- createRegionServerThread()
                      |         |    |   |    |- HregionServer.init()
                      |      - waitOnMasterThreads 
                      distributed
                      --------
                      |     - HMaster.constructMaster()
                      |         |
                      |         |- masterClass = org.apache.hadoop.hbase.master.HMaster
                      |         |    |- HMaster.init()
</code></pre>
<h2 id="2-流程详解">2.  流程详解</h2>
<ol>
<li>Hmaster 启动入口类为 hbase-server module 的 org.apache.hadoop.hbase.master.HMaster</li>
<li>在 HMaster 的启动过程中，会涉及到如下几个主要的类
<ol>
<li>org.apache.hadoop.hbase.master.HMaster
<ol>
<li>HMaster 核心类，继承于 org.apache.hadoop.hbase.regionserver.HRegionServer</li>
<li>在 HMaster 服务启动时会先执行 HRegionServer init 方法</li>
</ol>
</li>
<li>org.apache.hadoop.hbase.regionserver.HRegionServer
<ol>
<li>HRegionServer 核心类</li>
</ol>
</li>
<li>org.apache.hadoop.hbase.ipc.RpcServer
<ol>
<li>封装基于 PB 协议的 services，包括 Listener，Reader，Scheduler, Responder等</li>
</ol>
</li>
<li>org.apache.hadoop.hbase.ipc.RpcScheduler
<ol>
<li>rpc 请求调度算法接口，常见的实现类有org.apache.hadoop.hbase.ipc.SimpleRpcScheduler 和 org.apache.hadoop.hbase.ipc.FifoRpcScheduler<br>
<img src="https://casperit.github.io/post-images/1608209881964.png" alt="" loading="lazy"></li>
</ol>
</li>
</ol>
</li>
<li>核心代码实现如下:
<ol>
<li>调用入口类</li>
</ol>
<pre><code class="language-java">org.apache.hadoop.hbase.master.HMasterCommandLine#run


if (&quot;start&quot;.equals(command)) {
  return startMaster();
} else if (&quot;stop&quot;.equals(command)) {
  return stopMaster();
} else if (&quot;clear&quot;.equals(command)) {
  return (ZNodeClearer.clear(getConf()) ? 0 : 1);
} else {
  usage(&quot;Invalid command: &quot; + command);
  return 1;
}
</code></pre>
<ol start="2">
<li>startMaster</li>
</ol>
<pre><code class="language-java">org.apache.hadoop.hbase.master.HMasterCommandLine#startMaster

{
  // If 'local', defer to LocalHBaseCluster instance.  Starts master
  // and regionserver both in the one JVM.
  if (LocalHBaseCluster.isLocal(conf)) {
    
    // 启动minizk节点，只有一个实例
    final MiniZooKeeperCluster zooKeeperCluster = new MiniZooKeeperCluster(conf);
    int clientPort = zooKeeperCluster.startup(zkDataPath);

    LocalHBaseCluster cluster = new LocalHBaseCluster(conf, mastersCount, regionServersCount,
      LocalHMaster.class, HRegionServer.class);
    ((LocalHMaster)cluster.getMaster(0)).setZKCluster(zooKeeperCluster);
    cluster.startup();
    waitOnMasterThreads(cluster);
  } else {
    // 分布式模式下，启动HMaster类
    logProcessInfo(getConf());
    CoordinatedStateManager csm =
      CoordinatedStateManagerFactory.getCoordinatedStateManager(conf);
    HMaster master = HMaster.constructMaster(masterClass, conf, csm);
    if (master.isStopped()) {
      LOG.info(&quot;Won't bring the Master up as a shutdown is requested&quot;);
      return 1;
    }
    master.start();
    master.join();
    if(master.isAborted())
      throw new RuntimeException(&quot;HMaster Aborted&quot;);
  }
}
</code></pre>
<ol start="3">
<li>对于local集群，会在同一个 JVM 里面完成 LocalHMaster + RegionServer 实例的初始化操作</li>
<li>对于 LocalHMaster (实现类为 org.apache.hadoop.hbase.master.HMasterCommandLine$LocalHMaster ) 来说</li>
</ol>
<pre><code class="language-java">org.apache.hadoop.hbase.master.HMaster#HMaster

super(conf, csm);  // 调用 Hregionserver 的 init 方法


if (shouldPublish) {
  if (publisherClass == null) {
    LOG.warn(HConstants.STATUS_PUBLISHED + &quot; is true, but &quot; +
        ClusterStatusPublisher.DEFAULT_STATUS_PUBLISHER_CLASS +
        &quot; is not set - not publishing status&quot;);
  } else {
    clusterStatusPublisherChore = new ClusterStatusPublisher(this, conf, publisherClass);
    getChoreService().scheduleChore(clusterStatusPublisherChore); // clusterStatusPublisherChore 定时执行
  }
}

// Some unit tests don't need a cluster, so no zookeeper at all
if (!conf.getBoolean(&quot;hbase.testing.nocluster&quot;, false)) {
  setInitLatch(new CountDownLatch(1)); // 通过 CountDownLatch(1) 确保同一时间只有一个线程进入该方法
  // ActiveMasterManager 是一个 zookeeperListener 实现，负责master election
  activeMasterManager = new ActiveMasterManager(zooKeeper, this.serverName, this);
  int infoPort = putUpJettyServer();
  // 启动activemanager，尝试变成active manager，并启动相关后续服务
  startActiveMasterManager(infoPort);
} else {
  activeMasterManager = null;
}
</code></pre>
<ol start="5">
<li>HMaster 继承于 RegionServer，所以会先执行 RegionServer 的 init 的方法</li>
</ol>
<pre><code>org.apache.hadoop.hbase.regionserver.HRegionServer#HRegionServer

super(&quot;RegionServer&quot;);  // thread name

this.userProvider = UserProvider.instantiate(conf); // 初始化 user 实例
FSUtils.setupShortCircuitRead(this.conf);

this.nonceManager = isNoncesEnabled ? new ServerNonceManager(this.conf) : null; // nonceManager，通过 nonce 机制防止同一个请求被重复提交

rpcServices = createRpcServices(); //初始化 RpcServices 实例，包含scheduler,listener,responder和reader等对象

initializeFileSystem(); // 分别初始化wal和data HfileSystem，

service = new ExecutorService(getServerName().toShortString()); //初始化一个通用的executor service，包含一个threadpool和runnable
spanReceiverHost = SpanReceiverHost.getInstance(getConfiguration());

// Some unit tests don't need a cluster, so no zookeeper at all
if (!conf.getBoolean(&quot;hbase.testing.nocluster&quot;, false)) {
  // Open connection to zookeeper and set primary watcher
  zooKeeper = new ZooKeeperWatcher(conf, getProcessName() + &quot;:&quot; +
    rpcServices.isa.getPort(), this, canCreateBaseZNode());

  this.csm = (BaseCoordinatedStateManager) csm;
  this.csm.initialize(this);
  this.csm.start();

  tableLockManager = TableLockManager.createTableLockManager(
    conf, zooKeeper, serverName); //基于zk的tracker

  masterAddressTracker = new MasterAddressTracker(getZooKeeper(), this);
  masterAddressTracker.start(); //基于zk的tracker

  clusterStatusTracker = new ClusterStatusTracker(zooKeeper, this);
  clusterStatusTracker.start(); //基于zk的tracker
}
this.configurationManager = new ConfigurationManager(); // configurationManager to update certain configuration on the fly

rpcServices.start();  // start responder,listener,scheduler service
putUpWebUI(); // start http page
this.walRoller = new LogRoller(this, this); // wal rollger manager
this.choreService = new ChoreService(getServerName().toString(), true);
this.flushThroughputController = FlushThroughputControllerFactory.create(this, conf);

// 注册 HUP signal，用于 reload configuration 并通知相关 observer
if (!SystemUtils.IS_OS_WINDOWS) {
  Signal.handle(new Signal(&quot;HUP&quot;), new SignalHandler() {
    @Override
    public void handle(Signal signal) {
      getConfiguration().reloadConfiguration();
      configurationManager.notifyAllObservers(getConfiguration());
    }
  });
}
// Create the CompactedFileDischarger chore service. This chore helps to
// remove the compacted files
// that will no longer be used in reads.
// Default is 2 mins. The default value for TTLCleaner is 5 mins so we set this to
// 2 mins so that compacted files can be archived before the TTLCleaner runs
int cleanerInterval = conf
    .getInt(CompactionConfiguration.HBASE_HFILE_COMPACTION_DISCHARGER_INTERVAL, 2 * 60 * 1000);
this.compactedFileDischarger =
    new CompactedHFilesDischarger(cleanerInterval, (Stoppable)this, (RegionServerServices)this);
choreService.scheduleChore(compactedFileDischarger);
</code></pre>
<ol start="6">
<li>startActiveMasterManager 是比较核心的方法，它尝试把当前实例变成active master，并启动后续相关的服务</li>
</ol>
<pre><code class="language-java">org.apache.hadoop.hbase.master.HMaster#startActiveMasterManager

{

// 启动一个独立的线程用来尝试变成active master
Threads.setDaemonThreadRunning(new Thread(new Runnable() {
  @Override
  public void run() {
    int timeout = conf.getInt(HConstants.ZK_SESSION_TIMEOUT,
      HConstants.DEFAULT_ZK_SESSION_TIMEOUT);
    // If we're a backup master, stall until a primary to writes his address
    if (conf.getBoolean(HConstants.MASTER_TYPE_BACKUP,
      HConstants.DEFAULT_MASTER_TYPE_BACKUP)) {
      LOG.debug(&quot;HMaster started in backup mode. &quot;
        + &quot;Stalling until master znode is written.&quot;);
      // This will only be a minute or so while the cluster starts up,
      // so don't worry about setting watches on the parent znode
      while (!activeMasterManager.hasActiveMaster()) {
        LOG.debug(&quot;Waiting for master address ZNode to be written &quot;
          + &quot;(Also watching cluster state node)&quot;);
        Threads.sleep(timeout);
      }
    }
    MonitoredTask status = TaskMonitor.get().createStatus(&quot;Master startup&quot;);
    status.setDescription(&quot;Master startup&quot;);
    try {
        // 等待变成active，然后初始化后续的服务（通过在zk里面创建对应的znode节点而实现）
      if (activeMasterManager.blockUntilBecomingActiveMaster(timeout, status)) {
          // 变成active master后启动其他服务
        finishActiveMasterInitialization(status);
      }
    } catch (Throwable t) {
      status.setStatus(&quot;Failed to become active: &quot; + t.getMessage());
      LOG.fatal(&quot;Failed to become active master&quot;, t);
      // HBASE-5680: Likely hadoop23 vs hadoop 20.x/1.x incompatibility
      if (t instanceof NoClassDefFoundError &amp;&amp;
        t.getMessage()
          .contains(&quot;org/apache/hadoop/hdfs/protocol/HdfsConstants$SafeModeAction&quot;)) {
        // improved error message for this special case
        abort(&quot;HBase is having a problem with its Hadoop jars.  You may need to &quot;
          + &quot;recompile HBase against Hadoop version &quot;
          + org.apache.hadoop.util.VersionInfo.getVersion()
          + &quot; or change your hadoop jars to start properly&quot;, t);
      } else {
        abort(&quot;Unhandled exception. Starting shutdown.&quot;, t);
      }
    } finally {
      status.cleanup();
    }
  }
}, getServerName().toShortString() + &quot;.activeMasterManager&quot;));
}
</code></pre>
<ol start="7">
<li>在变成active master后，执行 finishActiveMasterInitialization，用来起来一些后续的服务</li>
</ol>
<pre><code class="language-java">org.apache.hadoop.hbase.master.HMaster#finishActiveMasterInitialization

{
 
/*
 * We are active master now... go initialize components we need to run.
 * Note, there may be dross in zk from previous runs; it'll get addressed
 * below after we determine if cluster startup or failover.
 */

status.setStatus(&quot;Initializing Master file system&quot;);


// MasterFileSystem ,用来负责 master 相关的 fs 操作
this.fileSystemManager = new MasterFileSystem(this, this);

// manages info about region servers
this.serverManager = createServerManager(this, this);

// Internal methods on Connection that should not be used by user code
setupClusterConnection();

status.setStatus(&quot;Initializing ZK system trackers&quot;);
// Initialize all ZK based system trackers，包括balancer, normalizer等
initializeZKBasedSystemTrackers();

// initialize master side coprocessors before we start handling requests
status.setStatus(&quot;Initializing master coprocessors&quot;);
this.cpHost = new MasterCoprocessorHost(this, this.conf);

// start up all service threads, 例如 MASTER_OPEN_REGION，MASTER_CLOSE_REGION 等
status.setStatus(&quot;Initializing master service threads&quot;);
startServiceThreads();

// Wait for region servers to report in
this.serverManager.waitForRegionServers(status);
// Check zk for region servers that are up but didn't register
for (ServerName sn: this.regionServerTracker.getOnlineServers()) {
  // The isServerOnline check is opportunistic, correctness is handled inside
  if (!this.serverManager.isServerOnline(sn)
      &amp;&amp; serverManager.checkAndRecordNewServer(sn, ServerLoad.EMPTY_SERVERLOAD)) {
    LOG.info(&quot;Registered server found up in zk but who has not yet reported in: &quot; + sn);
  }
}

// get a list for previously failed RS which need log splitting work
// we recover hbase:meta region servers inside master initialization and
// handle other failed servers in SSH in order to start up master node ASAP
Set&lt;ServerName&gt; previouslyFailedServers =
  this.fileSystemManager.getFailedServersFromLogFolders();

// log splitting for hbase:meta server
ServerName oldMetaServerLocation = metaTableLocator.getMetaRegionLocation(this.getZooKeeper());
if (oldMetaServerLocation != null &amp;&amp; previouslyFailedServers.contains(oldMetaServerLocation)) {
  splitMetaLogBeforeAssignment(oldMetaServerLocation);
  // Note: we can't remove oldMetaServerLocation from previousFailedServers list because it
  // may also host user regions
}

this.initializationBeforeMetaAssignment = true;

// Wait for regionserver to finish initialization.
if (BaseLoadBalancer.tablesOnMaster(conf)) {
  waitForServerOnline();
}

//initialize load balancer
this.balancer.setMasterServices(this);
this.balancer.setClusterStatus(getClusterStatusWithoutCoprocessor());
this.balancer.initialize();

// Make sure meta assigned before proceeding.
status.setStatus(&quot;Assigning Meta Region&quot;);
assignMeta(status, previouslyFailedMetaRSs, HRegionInfo.DEFAULT_REPLICA_ID);
// check if master is shutting down because above assignMeta could return even hbase:meta isn't
// assigned when master is shutting down
if (isStopped()) return;

status.setStatus(&quot;Submitting log splitting work for previously failed region servers&quot;);
// Master has recovered hbase:meta region server and we put
// other failed region servers in a queue to be handled later by SSH
for (ServerName tmpServer : previouslyFailedServers) {
  this.serverManager.processDeadServer(tmpServer, true);
}

// Fix up assignment manager status
status.setStatus(&quot;Starting assignment manager&quot;);
this.assignmentManager.joinCluster();

// Start balancer and meta catalog janitor after meta and regions have been assigned.
status.setStatus(&quot;Starting balancer and catalog janitor&quot;);
this.clusterStatusChore = new ClusterStatusChore(this, balancer);
getChoreService().scheduleChore(clusterStatusChore);
this.balancerChore = new BalancerChore(this);
getChoreService().scheduleChore(balancerChore);
this.normalizerChore = new RegionNormalizerChore(this);
getChoreService().scheduleChore(normalizerChore);
this.catalogJanitorChore = new CatalogJanitor(this, this);
getChoreService().scheduleChore(catalogJanitorChore);
periodicDoMetricsChore = new PeriodicDoMetrics(msgInterval, this);
getChoreService().scheduleChore(periodicDoMetricsChore);

status.setStatus(&quot;Starting namespace manager&quot;);
initNamespace();

status.markComplete(&quot;Initialization successful&quot;);
LOG.info(String.format(&quot;Master has completed initialization %.3fsec&quot;,
   (System.currentTimeMillis() - masterActiveTime) / 1000.0f));
this.masterFinishedInitializationTime = System.currentTimeMillis();
// 为如下几个服务注册configuration observer
configurationManager.registerObserver(this.balancer);
configurationManager.registerObserver(this.cleanerPool);
configurationManager.registerObserver(this.hfileCleaner);
configurationManager.registerObserver(this.logCleaner);

status.setStatus(&quot;Starting quota manager&quot;);
initQuotaManager();

// assign the meta replicas
Set&lt;ServerName&gt; EMPTY_SET = new HashSet&lt;ServerName&gt;();
int numReplicas = conf.getInt(HConstants.META_REPLICAS_NUM,
       HConstants.DEFAULT_META_REPLICA_NUM);
for (int i = 1; i &lt; numReplicas; i++) {
  assignMeta(status, EMPTY_SET, i);
}
}
</code></pre>
</li>
</ol>
]]></content>
    </entry>
</feed>