{"posts":[{"title":"Hmaster 启动过程","content":"1. 调用链 HMasterCommandLine | | - run local | ----- - startMaster -| | | |- new MiniZooKeeperCluster.startup //embedded zookeeper cluster | | |- createDir //创建 zk 数据目录 | | |- zks = new ZooKeeperServer | | |- new NIOServerCnxn.Factory(clientPort).startup //启动服务 | | | | | | | |- zks.startdata | | | | | | | | | |- new ZKDatabase | | | | |- loadData // Restore sessions and data | | | | | | | - zks.startup | | | | | | | |- startSessionTracker | | | |- setupRequestProcessors | | | | | | | |- new FinalRequestProcessor | | | |- new SyncRequestProcessor.start | | | |- new PrepRequestProcessor.start | | | |- registerJMX //注册JMX mbean | | | | | - waitForServerUp // echo &quot;stat&quot; to assert zk liveless | | | | | - new LocalHBaseCluster().startup //启动包括 Hmaster + RS 的服务 | | | |- masterClass = org.apache.hadoop.hbase.master.HMasterCommandLine$LocalHMaster | |- addMaster() | | |- createMasterThread() | | | |- HMaster.init() | | | | |- HregionServer.init() | | | | | |- UserProvider.instantiate(conf) // 初始化 userprovider | | | | | |- FSUtils.setupShortCircuitRead(this.conf) // short circuit read setup | | | | | |- createRpcServices() | | | | | | | - MasterRpcServices.init() -&gt;RSRpcServices.init() | | | | | | | | - rpcServer = new RpcServer() | | | | | | | | | - rpcSchedulerFactory.create() | | | | | | | | | | - callExecutor //初始化读写executor | | | | | | | | | | - priorityExecutor //初始化priorityExecutor | | | | | | | | | | - replicationExecutor //初始化replicationExecutor | | | | | |- new ZooKeeperWatcher() //初始化zk连接和watcher，并create相关的znode | | | | | | | - createBaseZNodes() | | | | | |- TableLockManager.createTableLockManager // 基于zk的table-level lock | | | | | |- new MasterAddressTracker.start() // znode watcher to track master address ( /hbase/master ) | | | | | |- new ClusterStatusTracker.start() // znode watcher to track cluster status ( /hbase/running ) | | | | | |- new ConfigurationManager() // reload configuration on the fly | | | | | |- rpcServices.start() // start rpc threads | | | | | | |- rpcServices.start() // start rpc threads | | | | | | | |- responder.start() | | | | | | | |- listener.start() | | | | | | | |- scheduler.start() | | | | | | | | | - callExecutor.start | | | | | | | | | - priorityExecutor.start | | | | | | | | | - replicationExecutor.start | | | | | | |- putUpWebUI() | | | | | | |- new LogRoller() | | | | | | |- new ChoreService() | | | | | - new ActiveMasterManager() // Handles everything on master-side related to master election. This is a zk listener | | | | | - startActiveMasterManager() | | | | | | - MasterAddressTracker.setMasterAddress() // create /hbase/backup-masters/XX | | | | | | - Threads.setDaemonThreadRunning() // Start a new thread to try to become the active master | | | | | | |- blockUntilBecomingActiveMaster() // try to become master | | | | | | | |- finishActiveMasterInitialization() | | | | | | | | | - new MasterFileSystem() // class for hmaster to interact with underlying file system | | | | | | | | | - new ServerManager() // manage info about region servers including online/dead servers and processing the startsup,shutdown and deaths of regionservers | | | | | | | | | - setupClusterConnection | | | | | | | | | - initializeZKBasedSystemTrackers // Initialize all ZK based system trackers | | | | | | | | | | - getLoadBalancer | | | | | | | | | | - getRegionNormalizer | | | | | | | | | | - new AssignmentManager | | | | | | | | | | - new SnapshotManager | | | | | | | | | | - new MasterProcedureManagerHost | | | | | | | | | - new MasterCoprocessorHost // master coprocessor class | | | | | | | | | - startServiceThreads // start several master executor service | | | | | | | | | | - MASTER_OPEN_REGION | | | | | | | | | | - MASTER_CLOSE_REGION | | | | | | | | | | - MASTER_SERVER_OPERATIONS | | | | | | | | | | - MASTER_META_SERVER_OPERATIONS | | | | | | | | | | - M_LOG_REPLAY_OPS | | | | | | | | | | - MASTER_SNAPSHOT_OPERATIONS | | | | | | | | | | - MASTER_TABLE_OPERATIONS | | | | | | | | | | - new LogCleaner chore | | | | | | | | | | - new HFileCleaner chore | | | | | | | | | | - new replicationZKLockCleanerChore chore | | | | | | | | | | - new replicationZKNodeCleanerChore chore | | | | | | | | | - assignMeta // assign meta table | | | | | | | | | - new ClusterStatusChore() | | | | | | | | | - new BalancerChore() | | | | | | | | | - new RegionNormalizerChore() | | | | | | | | | - new CatalogJanitor() | | | | | | | | | - new PeriodicDoMetrics() | | | | | | | | | - new TableNamespaceManager().start | | | | | | | | | - configurationManager.registerObserver(this.balancer) //enable configuration reload on the fly | | | | | | | | | - configurationManager.registerObserver(this.cleanerPool) | | | | | | | | | - configurationManager.registerObserver(this.hfileCleaner) | | | | | | | | | - configurationManager.registerObserver(this.logCleaner) | | | | | | | | | - initQuotaManager | | - RegionServerClass = org.apache.hadoop.hbase.regionserver.HRegionServer | | - addRegionServer () | | |- createRegionServerThread() | | | | |- HregionServer.init() | - waitOnMasterThreads distributed -------- | - HMaster.constructMaster() | | | |- masterClass = org.apache.hadoop.hbase.master.HMaster | | |- HMaster.init() 2. 流程详解 Hmaster 启动入口类为 hbase-server module 的 org.apache.hadoop.hbase.master.HMaster 在 HMaster 的启动过程中，会涉及到如下几个主要的类 org.apache.hadoop.hbase.master.HMaster HMaster 核心类，继承于 org.apache.hadoop.hbase.regionserver.HRegionServer 在 HMaster 服务启动时会先执行 HRegionServer init 方法 org.apache.hadoop.hbase.regionserver.HRegionServer HRegionServer 核心类 org.apache.hadoop.hbase.ipc.RpcServer 封装基于 PB 协议的 services，包括 Listener，Reader，Scheduler, Responder等 org.apache.hadoop.hbase.ipc.RpcScheduler rpc 请求调度算法接口，常见的实现类有org.apache.hadoop.hbase.ipc.SimpleRpcScheduler 和 org.apache.hadoop.hbase.ipc.FifoRpcScheduler 核心代码实现如下: 调用入口类 org.apache.hadoop.hbase.master.HMasterCommandLine#run if (&quot;start&quot;.equals(command)) { return startMaster(); } else if (&quot;stop&quot;.equals(command)) { return stopMaster(); } else if (&quot;clear&quot;.equals(command)) { return (ZNodeClearer.clear(getConf()) ? 0 : 1); } else { usage(&quot;Invalid command: &quot; + command); return 1; } startMaster org.apache.hadoop.hbase.master.HMasterCommandLine#startMaster { // If 'local', defer to LocalHBaseCluster instance. Starts master // and regionserver both in the one JVM. if (LocalHBaseCluster.isLocal(conf)) { // 启动minizk节点，只有一个实例 final MiniZooKeeperCluster zooKeeperCluster = new MiniZooKeeperCluster(conf); int clientPort = zooKeeperCluster.startup(zkDataPath); LocalHBaseCluster cluster = new LocalHBaseCluster(conf, mastersCount, regionServersCount, LocalHMaster.class, HRegionServer.class); ((LocalHMaster)cluster.getMaster(0)).setZKCluster(zooKeeperCluster); cluster.startup(); waitOnMasterThreads(cluster); } else { // 分布式模式下，启动HMaster类 logProcessInfo(getConf()); CoordinatedStateManager csm = CoordinatedStateManagerFactory.getCoordinatedStateManager(conf); HMaster master = HMaster.constructMaster(masterClass, conf, csm); if (master.isStopped()) { LOG.info(&quot;Won't bring the Master up as a shutdown is requested&quot;); return 1; } master.start(); master.join(); if(master.isAborted()) throw new RuntimeException(&quot;HMaster Aborted&quot;); } } 对于local集群，会在同一个 JVM 里面完成 LocalHMaster + RegionServer 实例的初始化操作 对于 LocalHMaster (实现类为 org.apache.hadoop.hbase.master.HMasterCommandLine$LocalHMaster ) 来说 org.apache.hadoop.hbase.master.HMaster#HMaster super(conf, csm); // 调用 Hregionserver 的 init 方法 if (shouldPublish) { if (publisherClass == null) { LOG.warn(HConstants.STATUS_PUBLISHED + &quot; is true, but &quot; + ClusterStatusPublisher.DEFAULT_STATUS_PUBLISHER_CLASS + &quot; is not set - not publishing status&quot;); } else { clusterStatusPublisherChore = new ClusterStatusPublisher(this, conf, publisherClass); getChoreService().scheduleChore(clusterStatusPublisherChore); // clusterStatusPublisherChore 定时执行 } } // Some unit tests don't need a cluster, so no zookeeper at all if (!conf.getBoolean(&quot;hbase.testing.nocluster&quot;, false)) { setInitLatch(new CountDownLatch(1)); // 通过 CountDownLatch(1) 确保同一时间只有一个线程进入该方法 // ActiveMasterManager 是一个 zookeeperListener 实现，负责master election activeMasterManager = new ActiveMasterManager(zooKeeper, this.serverName, this); int infoPort = putUpJettyServer(); // 启动activemanager，尝试变成active manager，并启动相关后续服务 startActiveMasterManager(infoPort); } else { activeMasterManager = null; } HMaster 继承于 RegionServer，所以会先执行 RegionServer 的 init 的方法 org.apache.hadoop.hbase.regionserver.HRegionServer#HRegionServer super(&quot;RegionServer&quot;); // thread name this.userProvider = UserProvider.instantiate(conf); // 初始化 user 实例 FSUtils.setupShortCircuitRead(this.conf); this.nonceManager = isNoncesEnabled ? new ServerNonceManager(this.conf) : null; // nonceManager，通过 nonce 机制防止同一个请求被重复提交 rpcServices = createRpcServices(); //初始化 RpcServices 实例，包含scheduler,listener,responder和reader等对象 initializeFileSystem(); // 分别初始化wal和data HfileSystem， service = new ExecutorService(getServerName().toShortString()); //初始化一个通用的executor service，包含一个threadpool和runnable spanReceiverHost = SpanReceiverHost.getInstance(getConfiguration()); // Some unit tests don't need a cluster, so no zookeeper at all if (!conf.getBoolean(&quot;hbase.testing.nocluster&quot;, false)) { // Open connection to zookeeper and set primary watcher zooKeeper = new ZooKeeperWatcher(conf, getProcessName() + &quot;:&quot; + rpcServices.isa.getPort(), this, canCreateBaseZNode()); this.csm = (BaseCoordinatedStateManager) csm; this.csm.initialize(this); this.csm.start(); tableLockManager = TableLockManager.createTableLockManager( conf, zooKeeper, serverName); //基于zk的tracker masterAddressTracker = new MasterAddressTracker(getZooKeeper(), this); masterAddressTracker.start(); //基于zk的tracker clusterStatusTracker = new ClusterStatusTracker(zooKeeper, this); clusterStatusTracker.start(); //基于zk的tracker } this.configurationManager = new ConfigurationManager(); // configurationManager to update certain configuration on the fly rpcServices.start(); // start responder,listener,scheduler service putUpWebUI(); // start http page this.walRoller = new LogRoller(this, this); // wal rollger manager this.choreService = new ChoreService(getServerName().toString(), true); this.flushThroughputController = FlushThroughputControllerFactory.create(this, conf); // 注册 HUP signal，用于 reload configuration 并通知相关 observer if (!SystemUtils.IS_OS_WINDOWS) { Signal.handle(new Signal(&quot;HUP&quot;), new SignalHandler() { @Override public void handle(Signal signal) { getConfiguration().reloadConfiguration(); configurationManager.notifyAllObservers(getConfiguration()); } }); } // Create the CompactedFileDischarger chore service. This chore helps to // remove the compacted files // that will no longer be used in reads. // Default is 2 mins. The default value for TTLCleaner is 5 mins so we set this to // 2 mins so that compacted files can be archived before the TTLCleaner runs int cleanerInterval = conf .getInt(CompactionConfiguration.HBASE_HFILE_COMPACTION_DISCHARGER_INTERVAL, 2 * 60 * 1000); this.compactedFileDischarger = new CompactedHFilesDischarger(cleanerInterval, (Stoppable)this, (RegionServerServices)this); choreService.scheduleChore(compactedFileDischarger); startActiveMasterManager 是比较核心的方法，它尝试把当前实例变成active master，并启动后续相关的服务 org.apache.hadoop.hbase.master.HMaster#startActiveMasterManager { // 启动一个独立的线程用来尝试变成active master Threads.setDaemonThreadRunning(new Thread(new Runnable() { @Override public void run() { int timeout = conf.getInt(HConstants.ZK_SESSION_TIMEOUT, HConstants.DEFAULT_ZK_SESSION_TIMEOUT); // If we're a backup master, stall until a primary to writes his address if (conf.getBoolean(HConstants.MASTER_TYPE_BACKUP, HConstants.DEFAULT_MASTER_TYPE_BACKUP)) { LOG.debug(&quot;HMaster started in backup mode. &quot; + &quot;Stalling until master znode is written.&quot;); // This will only be a minute or so while the cluster starts up, // so don't worry about setting watches on the parent znode while (!activeMasterManager.hasActiveMaster()) { LOG.debug(&quot;Waiting for master address ZNode to be written &quot; + &quot;(Also watching cluster state node)&quot;); Threads.sleep(timeout); } } MonitoredTask status = TaskMonitor.get().createStatus(&quot;Master startup&quot;); status.setDescription(&quot;Master startup&quot;); try { // 等待变成active，然后初始化后续的服务（通过在zk里面创建对应的znode节点而实现） if (activeMasterManager.blockUntilBecomingActiveMaster(timeout, status)) { // 变成active master后启动其他服务 finishActiveMasterInitialization(status); } } catch (Throwable t) { status.setStatus(&quot;Failed to become active: &quot; + t.getMessage()); LOG.fatal(&quot;Failed to become active master&quot;, t); // HBASE-5680: Likely hadoop23 vs hadoop 20.x/1.x incompatibility if (t instanceof NoClassDefFoundError &amp;&amp; t.getMessage() .contains(&quot;org/apache/hadoop/hdfs/protocol/HdfsConstants$SafeModeAction&quot;)) { // improved error message for this special case abort(&quot;HBase is having a problem with its Hadoop jars. You may need to &quot; + &quot;recompile HBase against Hadoop version &quot; + org.apache.hadoop.util.VersionInfo.getVersion() + &quot; or change your hadoop jars to start properly&quot;, t); } else { abort(&quot;Unhandled exception. Starting shutdown.&quot;, t); } } finally { status.cleanup(); } } }, getServerName().toShortString() + &quot;.activeMasterManager&quot;)); } 在变成active master后，执行 finishActiveMasterInitialization，用来起来一些后续的服务 org.apache.hadoop.hbase.master.HMaster#finishActiveMasterInitialization { /* * We are active master now... go initialize components we need to run. * Note, there may be dross in zk from previous runs; it'll get addressed * below after we determine if cluster startup or failover. */ status.setStatus(&quot;Initializing Master file system&quot;); // MasterFileSystem ,用来负责 master 相关的 fs 操作 this.fileSystemManager = new MasterFileSystem(this, this); // manages info about region servers this.serverManager = createServerManager(this, this); // Internal methods on Connection that should not be used by user code setupClusterConnection(); status.setStatus(&quot;Initializing ZK system trackers&quot;); // Initialize all ZK based system trackers，包括balancer, normalizer等 initializeZKBasedSystemTrackers(); // initialize master side coprocessors before we start handling requests status.setStatus(&quot;Initializing master coprocessors&quot;); this.cpHost = new MasterCoprocessorHost(this, this.conf); // start up all service threads, 例如 MASTER_OPEN_REGION，MASTER_CLOSE_REGION 等 status.setStatus(&quot;Initializing master service threads&quot;); startServiceThreads(); // Wait for region servers to report in this.serverManager.waitForRegionServers(status); // Check zk for region servers that are up but didn't register for (ServerName sn: this.regionServerTracker.getOnlineServers()) { // The isServerOnline check is opportunistic, correctness is handled inside if (!this.serverManager.isServerOnline(sn) &amp;&amp; serverManager.checkAndRecordNewServer(sn, ServerLoad.EMPTY_SERVERLOAD)) { LOG.info(&quot;Registered server found up in zk but who has not yet reported in: &quot; + sn); } } // get a list for previously failed RS which need log splitting work // we recover hbase:meta region servers inside master initialization and // handle other failed servers in SSH in order to start up master node ASAP Set&lt;ServerName&gt; previouslyFailedServers = this.fileSystemManager.getFailedServersFromLogFolders(); // log splitting for hbase:meta server ServerName oldMetaServerLocation = metaTableLocator.getMetaRegionLocation(this.getZooKeeper()); if (oldMetaServerLocation != null &amp;&amp; previouslyFailedServers.contains(oldMetaServerLocation)) { splitMetaLogBeforeAssignment(oldMetaServerLocation); // Note: we can't remove oldMetaServerLocation from previousFailedServers list because it // may also host user regions } this.initializationBeforeMetaAssignment = true; // Wait for regionserver to finish initialization. if (BaseLoadBalancer.tablesOnMaster(conf)) { waitForServerOnline(); } //initialize load balancer this.balancer.setMasterServices(this); this.balancer.setClusterStatus(getClusterStatusWithoutCoprocessor()); this.balancer.initialize(); // Make sure meta assigned before proceeding. status.setStatus(&quot;Assigning Meta Region&quot;); assignMeta(status, previouslyFailedMetaRSs, HRegionInfo.DEFAULT_REPLICA_ID); // check if master is shutting down because above assignMeta could return even hbase:meta isn't // assigned when master is shutting down if (isStopped()) return; status.setStatus(&quot;Submitting log splitting work for previously failed region servers&quot;); // Master has recovered hbase:meta region server and we put // other failed region servers in a queue to be handled later by SSH for (ServerName tmpServer : previouslyFailedServers) { this.serverManager.processDeadServer(tmpServer, true); } // Fix up assignment manager status status.setStatus(&quot;Starting assignment manager&quot;); this.assignmentManager.joinCluster(); // Start balancer and meta catalog janitor after meta and regions have been assigned. status.setStatus(&quot;Starting balancer and catalog janitor&quot;); this.clusterStatusChore = new ClusterStatusChore(this, balancer); getChoreService().scheduleChore(clusterStatusChore); this.balancerChore = new BalancerChore(this); getChoreService().scheduleChore(balancerChore); this.normalizerChore = new RegionNormalizerChore(this); getChoreService().scheduleChore(normalizerChore); this.catalogJanitorChore = new CatalogJanitor(this, this); getChoreService().scheduleChore(catalogJanitorChore); periodicDoMetricsChore = new PeriodicDoMetrics(msgInterval, this); getChoreService().scheduleChore(periodicDoMetricsChore); status.setStatus(&quot;Starting namespace manager&quot;); initNamespace(); status.markComplete(&quot;Initialization successful&quot;); LOG.info(String.format(&quot;Master has completed initialization %.3fsec&quot;, (System.currentTimeMillis() - masterActiveTime) / 1000.0f)); this.masterFinishedInitializationTime = System.currentTimeMillis(); // 为如下几个服务注册configuration observer configurationManager.registerObserver(this.balancer); configurationManager.registerObserver(this.cleanerPool); configurationManager.registerObserver(this.hfileCleaner); configurationManager.registerObserver(this.logCleaner); status.setStatus(&quot;Starting quota manager&quot;); initQuotaManager(); // assign the meta replicas Set&lt;ServerName&gt; EMPTY_SET = new HashSet&lt;ServerName&gt;(); int numReplicas = conf.getInt(HConstants.META_REPLICAS_NUM, HConstants.DEFAULT_META_REPLICA_NUM); for (int i = 1; i &lt; numReplicas; i++) { assignMeta(status, EMPTY_SET, i); } } startActiveMasterManager 是比较核心的方法，它尝试把当前实例变成active master，并启动后续相关的服务 ","link":"https://casperit.github.io/post/hmaster-qi-dong-guo-cheng/"}]}